{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07686ef-6db7-4c8d-a051-240809c92cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from nltk import flatten\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from nltk import flatten\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d95d62-a194-4458-9156-e9916425eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e4a3810-ef4d-4829-9b97-7bd875403935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_HTML(text):\n",
    "    return re.sub(r'<[^>]*>', '', text) # Thay tháº¿ cÃ¡c kÃ½ tá»± á»Ÿ param 1 báº±ng kÃ½ tá»± á»Ÿ param 2 trong chuá»—i kÃ½ tá»± text\n",
    "\n",
    "\n",
    "# Chuáº©n hoÃ¡ unicode, chuyá»ƒn cÃ¡c kÃ½ tá»± tá»« mÃ£ window 1252 vá» dáº¡ng utf8\n",
    "def convert_unicode(text):\n",
    "    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'\n",
    "    charutf8 = 'Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´'\n",
    "    char1252 = char1252.split('|')\n",
    "    charutf8 = charutf8.split('|')\n",
    "\n",
    "    dic = {}\n",
    "    for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]\n",
    "    return re.sub(\n",
    "        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',\n",
    "        lambda x: dic[x.group()], text\n",
    "    )\n",
    "\n",
    "\n",
    "# Chuáº©n hoÃ¡ cÃ¡ch typing. Tráº£ ra tá»« Ä‘iá»ƒn vowels_to_ids Ä‘á»ƒ tÃ¬m ra vá»‹ trÃ­ cá»§a báº¥t ká»³ nguyÃªn Ã¢m cÃ³ dáº¥u nÃ o trong báº£ng vowels_table vÃ  thá»±c hiá»‡n cÃ¡c thao tÃ¡c xá»­ lÃ½ vÄƒn báº£n tiáº¿p theo dá»±a trÃªn Ä‘Ã³\n",
    "vowels_to_ids = {}\n",
    "vowels_table = [\n",
    "    ['a', 'Ã ', 'Ã¡', 'áº£', 'Ã£', 'áº¡', 'a' ],\n",
    "    ['Äƒ', 'áº±', 'áº¯', 'áº³', 'áºµ', 'áº·', 'aw'],\n",
    "    ['Ã¢', 'áº§', 'áº¥', 'áº©', 'áº«', 'áº­', 'aa'],\n",
    "    ['e', 'Ã¨', 'Ã©', 'áº»', 'áº½', 'áº¹', 'e' ],\n",
    "    ['Ãª', 'á»', 'áº¿', 'á»ƒ', 'á»…', 'á»‡', 'ee'],\n",
    "    ['i', 'Ã¬', 'Ã­', 'á»‰', 'Ä©', 'á»‹', 'i' ],\n",
    "    ['o', 'Ã²', 'Ã³', 'á»', 'Ãµ', 'á»', 'o' ],\n",
    "    ['Ã´', 'á»“', 'á»‘', 'á»•', 'á»—', 'á»™', 'oo'],\n",
    "    ['Æ¡', 'á»', 'á»›', 'á»Ÿ', 'á»¡', 'á»£', 'ow'],\n",
    "    ['u', 'Ã¹', 'Ãº', 'á»§', 'Å©', 'á»¥', 'u' ],\n",
    "    ['Æ°', 'á»«', 'á»©', 'á»­', 'á»¯', 'á»±', 'uw'],\n",
    "    ['y', 'á»³', 'Ã½', 'á»·', 'á»¹', 'á»µ', 'y' ]\n",
    "]\n",
    "\n",
    "for i in range(len(vowels_table)):\n",
    "    for j in range(len(vowels_table[i]) - 1):\n",
    "        vowels_to_ids[vowels_table[i][j]] = (i, j)\n",
    "\n",
    "#Kiá»ƒm tra cÃ³ pháº£i chá»¯ tiáº¿ng Viá»‡t khÃ´ng dá»±a trÃªn tá»« Ä‘iá»ƒn vowels_to_ids\n",
    "def is_valid_vietnamese_word(word):\n",
    "    chars = list(word)\n",
    "    vowel_indexes = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if vowel_indexes == -1: vowel_indexes = index\n",
    "            else:\n",
    "                if index - vowel_indexes != 1: return False\n",
    "                vowel_indexes = index\n",
    "    return True\n",
    "\n",
    "# Chuáº©n hoÃ¡ chá»¯ viáº¿t\n",
    "def standardize_word_typing(word):\n",
    "    if not is_valid_vietnamese_word(word): return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    vowel_indexes = []\n",
    "    qu_or_gi = False\n",
    "\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        elif x == 9:  # check phá»¥ Ã¢m 'qu'\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check phá»¥ Ã¢m 'gi'\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = vowels_table[x][0]\n",
    "\n",
    "        if not qu_or_gi or index != 1:\n",
    "            vowel_indexes.append(index)\n",
    "\n",
    "    if len(vowel_indexes) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = vowels_to_ids.get(chars[1])\n",
    "                chars[1] = vowels_table[x][dau_cau]\n",
    "            else:\n",
    "                x, y = vowels_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1: chars[2] = vowels_table[x][dau_cau]\n",
    "                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in vowel_indexes:\n",
    "        x, y = vowels_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # Ãª, Æ¡\n",
    "            chars[index] = vowels_table[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(vowel_indexes) == 2:\n",
    "        if vowel_indexes[-1] == len(chars) - 1:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[0]]]\n",
    "            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]\n",
    "        else:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    else:\n",
    "        x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "# Chuáº©n hoÃ¡ cÃ¢u viáº¿t\n",
    "def standardize_sentence_typing(text):\n",
    "    words = text.lower().split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "replace_list = {\n",
    "    'Ã´ kÃªi': 'ok', 'okie': 'ok', 'o kÃª': 'ok', 'okey': 'ok', 'Ã´kÃª': 'ok', 'oki': 'ok', 'oke': 'ok', 'okay': 'ok', 'okÃª': 'ok',\n",
    "    'tks': 'cáº£m Æ¡n', 'thks': 'cáº£m Æ¡n', 'thanks': 'cáº£m Æ¡n', 'ths': 'cáº£m Æ¡n', 'thank': 'cáº£m Æ¡n',\n",
    "    'kg': 'khÃ´ng', 'not': 'khÃ´ng', 'k': 'khÃ´ng', 'kh': 'khÃ´ng', 'kÃ´': 'khÃ´ng', 'hok': 'khÃ´ng', 'ko': 'khÃ´ng', 'khong': 'khÃ´ng', 'kp': 'khÃ´ng pháº£i',\n",
    "    'he he': 'tÃ­ch cá»±c', 'hehe': 'tÃ­ch cá»±c', 'hihi': 'tÃ­ch cá»±c', 'haha': 'tÃ­ch cá»±c', 'hjhj': 'tÃ­ch cá»±c', 'thick': 'tÃ­ch cá»±c',\n",
    "    'lol': 'tiÃªu cá»±c', 'cc': 'tiÃªu cá»±c', 'huhu': 'tiÃªu cá»±c', 'cute': 'dá»… thÆ°Æ¡ng',\n",
    "\n",
    "    'sz': 'cá»¡', 'size': 'cá»¡',\n",
    "    'wa': 'quÃ¡', 'wÃ¡': 'quÃ¡', 'qÃ¡': 'quÃ¡',\n",
    "    'Ä‘x': 'Ä‘Æ°á»£c', 'dk': 'Ä‘Æ°á»£c', 'dc': 'Ä‘Æ°á»£c', 'Ä‘k': 'Ä‘Æ°á»£c', 'Ä‘c': 'Ä‘Æ°á»£c',\n",
    "    'vs': 'vá»›i', 'j': 'gÃ¬', 'â€œ': ' ', 'time': 'thá»i gian', 'm': 'mÃ¬nh', 'mik': 'mÃ¬nh', 'r': 'rá»“i', 'bjo': 'bao giá»', 'very': 'ráº¥t',\n",
    "\n",
    "    'authentic': 'chuáº©n chÃ­nh hÃ£ng', 'aut': 'chuáº©n chÃ­nh hÃ£ng', 'auth': 'chuáº©n chÃ­nh hÃ£ng', 'date': 'háº¡n sá»­ dá»¥ng', 'hsd': 'háº¡n sá»­ dá»¥ng',\n",
    "    'store': 'cá»­a hÃ ng', 'sop': 'cá»­a hÃ ng', 'shopE': 'cá»­a hÃ ng', 'shop': 'cá»­a hÃ ng',\n",
    "    'sp': 'sáº£n pháº©m', 'product': 'sáº£n pháº©m', 'hÃ g': 'hÃ ng',\n",
    "    'ship': 'giao hÃ ng', 'delivery': 'giao hÃ ng', 'sÃ­p': 'giao hÃ ng', 'order': 'Ä‘áº·t hÃ ng',\n",
    "\n",
    "    'gud': 'tá»‘t', 'wel done': 'tá»‘t', 'good': 'tá»‘t', 'gÃºt': 'tá»‘t', 'tot': 'tá»‘t', 'nice': 'tá»‘t', 'perfect': 'ráº¥t tá»‘t',\n",
    "    'quality': 'cháº¥t lÆ°á»£ng', 'cháº¥t lg': 'cháº¥t lÆ°á»£ng', 'chat': 'cháº¥t', 'excelent': 'hoÃ n háº£o', 'bt': 'bÃ¬nh thÆ°á»ng',\n",
    "    'sad': 'tá»‡', 'por': 'tá»‡', 'poor': 'tá»‡', 'bad': 'tá»‡',\n",
    "    'beautiful': 'Ä‘áº¹p tuyá»‡t vá»i', 'dep': 'Ä‘áº¹p',\n",
    "    'xau': 'xáº¥u', 'sáº¥u': 'xáº¥u',\n",
    "\n",
    "    'thik': 'thÃ­ch', 'iu': 'yÃªu', 'fake': 'giáº£ máº¡o',\n",
    "    'quickly': 'nhanh', 'quick': 'nhanh', 'fast': 'nhanh',\n",
    "    'fresh': 'tÆ°Æ¡i', 'delicious': 'ngon',\n",
    "\n",
    "    'dt': 'Ä‘iá»‡n thoáº¡i', 'fb': 'facebook', 'face': 'facebook', 'ks': 'khÃ¡ch sáº¡n', 'nv': 'nhÃ¢n viÃªn',\n",
    "    'nt': 'nháº¯n tin', 'ib': 'nháº¯n tin', 'tl': 'tráº£ lá»i', 'trl': 'tráº£ lá»i', 'rep': 'tráº£ lá»i',\n",
    "    'fback': 'feedback', 'fedback': 'feedback',\n",
    "    'sd': 'sá»­ dá»¥ng', 'sÃ i': 'xÃ i',\n",
    "\n",
    "    '^_^': 'tÃ­ch cá»±c', ':)': 'tÃ­ch cá»±c', ':(': 'tiÃªu cá»±c',\n",
    "    'â¤ï¸': 'tÃ­ch cá»±c', 'ğŸ‘': 'tÃ­ch cá»±c', 'ğŸ‰': 'tÃ­ch cá»±c', 'ğŸ˜€': 'tÃ­ch cá»±c', 'ğŸ˜': 'tÃ­ch cá»±c', 'ğŸ˜‚': 'tÃ­ch cá»±c', 'ğŸ¤—': 'tÃ­ch cá»±c', 'ğŸ˜™': 'tÃ­ch cá»±c', 'ğŸ™‚': 'tÃ­ch cá»±c',\n",
    "    'ğŸ˜”': 'tiÃªu cá»±c', 'ğŸ˜“': 'tiÃªu cá»±c',\n",
    "    'â­': 'star', '*': 'star', 'ğŸŒŸ': 'star',\n",
    "}\n",
    "\n",
    "# Chuáº©n hoÃ¡ cÃ¡c tá»« viáº¿t táº¯t, kÃ½ tá»± thÃ nh tiáº¿ng Viá»‡t dá»±a trÃªn danh sÃ¡ch replace_list\n",
    "def normalize_acronyms(text):\n",
    "    words = []\n",
    "    for word in text.strip().split():\n",
    "        # word = word.strip(string.punctuation)\n",
    "        if word.lower() not in replace_list.keys(): words.append(word)\n",
    "        else: words.append(replace_list[word.lower()])\n",
    "    return emoji.demojize(' '.join(words)) # Remove Emojis\n",
    "\n",
    "\n",
    "# Word segmentation: Tráº£ ra má»™t chuá»—i vÄƒn báº£n vá»›i cÃ¡c tá»« Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n tÃ¡ch vÃ  ná»‘i láº¡i vá»›i nhau báº±ng khoáº£ng tráº¯ng, sá»­ dá»¥ng nguá»“n file chuáº©n hoÃ¡ VnCoreNLP-1.1.1.jar\n",
    "#annotator = VnCoreNLP('VnCoreNLP/VnCoreNLP-1.1.1.jar')\n",
    "#def word_segmentation(text):\n",
    "#    words = annotator.tokenize(text)\n",
    "#    return ' '.join(word for word in flatten(words))\n",
    "\n",
    "\n",
    "# Loáº¡i bá» cÃ¡c kÃ½ tá»± khÃ´ng cáº§n thiáº¿t trong vÄƒn báº£n\n",
    "def remove_unnecessary_characters(text):\n",
    "    text = re.sub(r'[^\\s\\wÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†Ã“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃÃŒá»ˆÄ¨á»ŠÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä_]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text\n",
    "# Táº¡o function tá»•ng há»£p xá»­ lÃ½ 1 vÄƒn báº£n\n",
    "def text_preprocess(text):\n",
    "    text = remove_HTML(text)\n",
    "    text = convert_unicode(text)\n",
    "    # text = standardize_sentence_typing(text)\n",
    "    text = normalize_acronyms(text)\n",
    "    #text = word_segmentation(text) # When use PhoBERT\n",
    "    text = remove_unnecessary_characters(text)\n",
    "    # return text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "941496fe-057c-4fa9-aff5-5559c3c1184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Admin\\Documents\\Zalo Received Files\\VLSP_Hotel'\n",
    "os.chdir(path)\n",
    "TRAIN_PATH = 'hotel_train.csv'\n",
    "VAL_PATH = 'hotel_val.csv'\n",
    "TEST_PATH = 'hotel_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d6096b-234f-4a49-aabd-32703715bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_PATH).head(200)\n",
    "df_val = pd.read_csv(VAL_PATH).head(50)\n",
    "df_test = pd.read_csv(TEST_PATH).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68341003-c111-4806-b408-b39d3a155e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['Review'] = df_val['Review'].apply(lambda x: text_preprocess(str(x)))\n",
    "df_test['Review'] = df_test['Review'].apply(lambda x: text_preprocess(str(x)))\n",
    "df_train['Review'] = df_train['Review'].apply(lambda x: text_preprocess(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0699e9b-7641-4bec-8f6f-f95361529ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot_and_filter(df):\n",
    "    # Unpivot DataFrame\n",
    "    df_unpivoted = pd.melt(df, id_vars=['Review'], var_name='Aspect', value_name='Sentiment')\n",
    "    \n",
    "    # Loáº¡i bá» cÃ¡c dÃ²ng cÃ³ sentiment \"none\"\n",
    "    df_filtered = df_unpivoted #[df_unpivoted['Sentiment'] != 0]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Ãp dá»¥ng function cho cÃ¡c DataFrame\n",
    "df_train = unpivot_and_filter(df_train).reset_index()\n",
    "df_val = unpivot_and_filter(df_val).reset_index()\n",
    "df_test = unpivot_and_filter(df_test).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "400e0133-72ba-4465-bbba-2968dbdaf9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = str(self.data.loc[index, 'Review'])\n",
    "        aspect = str(self.data.loc[index, 'Aspect'])\n",
    "        sentiment = self.data.loc[index, 'Sentiment']\n",
    "        \n",
    "        # Encode sentiment\n",
    "        if sentiment == 1: one_hot = [0, 1, 0, 0] # Positive\n",
    "        elif sentiment == 2: one_hot = [0, 0, 1, 0] # Negatvie\n",
    "        elif sentiment == 3: one_hot = [0, 0, 0, 1] # Neutral\n",
    "        else: one_hot = [1, 0, 0, 0]  # Default to none if sentiment is missing or incorrect\n",
    "            \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review + \" \" + aspect,  # Concatenate review and aspect\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(one_hot, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Táº¡o DataLoader\n",
    "batch_size = 4\n",
    "max_len = 256\n",
    "train_dataset = CustomDataset(df_train, tokenizer, max_len=max_len)\n",
    "val_dataset = CustomDataset(df_val, tokenizer, max_len=max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dedd070e-cc26-40dc-93b2-2f913c19609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, pooled_output = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.classifier(output)\n",
    "\n",
    "model = CustomModel(num_labels=4)  # Adjust num_labels to 4 for Positive, Negative, Neutral, None\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Move model to the device\n",
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87497a7e-428a-4128-ae79-9f69f03f3435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   565, 10456,  ...,     1,     1,     1],\n",
      "        [    0,   448,  3849,  ...,  3070,  6248,     2],\n",
      "        [    0,   487,   298,  ...,     1,     1,     1],\n",
      "        [    0, 17297,  3849,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "# In ra má»™t batch Ä‘á»ƒ kiá»ƒm tra\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87776c3d-b7b6-4162-926e-4a2fe0a4ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    train_loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] finished with average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return train_loss_history\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss_history = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    val_loss_history.append(epoch_val_loss)\n",
    "    print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    return val_loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4d8e2-2c32-45b8-bffb-59761a15524d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577ac8d2-63a5-4aa9-bb7f-0bda4aa4262d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_loss_history \u001b[38;5;241m=\u001b[39m train(model, train_loader, criterion, optimizer, epochs)\n\u001b[0;32m      3\u001b[0m val_loss_history \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask, token_type_ids)\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, token_type_ids):\n\u001b[1;32m---> 12\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[0;32m     13\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m     14\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m     15\u001b[0m         token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m     16\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(pooled_output)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(output)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:828\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    819\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    821\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    822\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    823\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    826\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    827\u001b[0m )\n\u001b[1;32m--> 828\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    829\u001b[0m     embedding_output,\n\u001b[0;32m    830\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    831\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    832\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    833\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m    834\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    835\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    836\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    837\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    838\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    839\u001b[0m )\n\u001b[0;32m    840\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    841\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:517\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    506\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    507\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    508\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m         output_attentions,\n\u001b[0;32m    515\u001b[0m     )\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 517\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    518\u001b[0m         hidden_states,\n\u001b[0;32m    519\u001b[0m         attention_mask,\n\u001b[0;32m    520\u001b[0m         layer_head_mask,\n\u001b[0;32m    521\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    522\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    523\u001b[0m         past_key_value,\n\u001b[0;32m    524\u001b[0m         output_attentions,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    527\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:406\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    396\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    407\u001b[0m         hidden_states,\n\u001b[0;32m    408\u001b[0m         attention_mask,\n\u001b[0;32m    409\u001b[0m         head_mask,\n\u001b[0;32m    410\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    411\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    413\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:333\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    325\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    332\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 333\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    334\u001b[0m         hidden_states,\n\u001b[0;32m    335\u001b[0m         attention_mask,\n\u001b[0;32m    336\u001b[0m         head_mask,\n\u001b[0;32m    337\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    338\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    339\u001b[0m         past_key_value,\n\u001b[0;32m    340\u001b[0m         output_attentions,\n\u001b[0;32m    341\u001b[0m     )\n\u001b[0;32m    342\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    343\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:259\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    256\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1813\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1809\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m-> 1813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   1814\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies a softmax function.\u001b[39;00m\n\u001b[0;32m   1815\u001b[0m \n\u001b[0;32m   1816\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1836\u001b[0m \n\u001b[0;32m   1837\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "train_loss_history = train(model, train_loader, criterion, optimizer, epochs)\n",
    "val_loss_history = evaluate(model, val_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "914208cf-0503-4b73-be86-d33ab90487db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = 'model_weights.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70b245-64a6-4583-ba0e-40c4597806c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import re\n",
    "model_name = \"bert-base-uncased\"  # Sá»­ dá»¥ng mÃ´ hÃ¬nh BERT\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=34*4)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Äá»‹nh nghÄ©a cÃ¡c danh má»¥c tá»« dataframe Ä‘Ã£ unpivot\n",
    "categories = ['FACILITIES#CLEANLINESS', 'FACILITIES#COMFORT', 'FACILITIES#DESIGN&FEATURES', \n",
    "              'FACILITIES#GENERAL', 'FACILITIES#MISCELLANEOUS', 'FACILITIES#PRICES', \n",
    "              'FACILITIES#QUALITY', 'FOOD&DRINKS#MISCELLANEOUS', 'FOOD&DRINKS#PRICES', \n",
    "              'FOOD&DRINKS#QUALITY', 'FOOD&DRINKS#STYLE&OPTIONS', 'HOTEL#CLEANLINESS', \n",
    "              'HOTEL#COMFORT', 'HOTEL#DESIGN&FEATURES', 'HOTEL#GENERAL', 'HOTEL#MISCELLANEOUS', \n",
    "              'HOTEL#PRICES', 'HOTEL#QUALITY', 'LOCATION#GENERAL', 'ROOM_AMENITIES#CLEANLINESS', \n",
    "              'ROOM_AMENITIES#COMFORT', 'ROOM_AMENITIES#DESIGN&FEATURES', 'ROOM_AMENITIES#GENERAL', \n",
    "              'ROOM_AMENITIES#MISCELLANEOUS', 'ROOM_AMENITIES#PRICES', 'ROOM_AMENITIES#QUALITY', \n",
    "              'ROOMS#CLEANLINESS', 'ROOMS#COMFORT', 'ROOMS#DESIGN&FEATURES', 'ROOMS#GENERAL', \n",
    "              'ROOMS#MISCELLANEOUS', 'ROOMS#PRICES', 'ROOMS#QUALITY', 'SERVICE#GENERAL']\n",
    "# HÃ m dá»± Ä‘oÃ¡n\n",
    "def predict(model, tokenizer, sentence):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        max_length=128,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        predictions = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# HÃ m decode_predictions Ä‘á»ƒ giáº£i mÃ£ cÃ¡c dá»± Ä‘oÃ¡n vÃ  tráº£ vá» danh sÃ¡ch cÃ¡c aspect vÃ  sentiment tÆ°Æ¡ng á»©ng\n",
    "def decode_predictions(predicted_labels, threshold=0.5):\n",
    "    sentiments = ['positive', 'negative', 'neutral', 'none']\n",
    "    decoded_predictions = {}\n",
    "    for i, pred in enumerate(predicted_labels[0]):\n",
    "        if pred > threshold:\n",
    "            sentiment = sentiments[i % 4]\n",
    "            if sentiment in ['neutral', 'none']:\n",
    "                continue  # Bá» qua náº¿u sentiment lÃ  neutral hoáº·c none\n",
    "            category_index = i // 4\n",
    "            category = categories[category_index]\n",
    "            if category not in decoded_predictions or pred > decoded_predictions[category][1]:\n",
    "                decoded_predictions[category] = (sentiment, pred)\n",
    "    \n",
    "    return [f\"{category}: {sentiment}\" for category, (sentiment, _) in decoded_predictions.items()]\n",
    "\n",
    "# Nháº­n vÄƒn báº£n Ä‘áº§u vÃ o tá»« ngÆ°á»i dÃ¹ng\n",
    "input_text = input(\"Nháº­p vÄƒn báº£n: \")\n",
    "example_sentence = text_preprocess(input_text)\n",
    "\n",
    "# Dá»± Ä‘oÃ¡n nhÃ£n\n",
    "predicted_labels = predict(model, tokenizer, example_sentence)\n",
    "\n",
    "# Giáº£i mÃ£ dá»± Ä‘oÃ¡n\n",
    "decoded_predictions = decode_predictions(predicted_labels)\n",
    "\n",
    "# Hiá»ƒn thá»‹ káº¿t quáº£ dá»± Ä‘oÃ¡n\n",
    "print(f\"Example: {input_text}\")\n",
    "for prediction in decoded_predictions:\n",
    "    print(f\"=> {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3da8dfe-d205-4da7-8f86-c17a956803ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6431    0.6431    0.6431       737\n",
      "    positive     0.3490    0.3490    0.3490       404\n",
      "\n",
      "    accuracy                         0.5390      1141\n",
      "   macro avg     0.4961    0.4961    0.4961      1141\n",
      "weighted avg     0.5390    0.5390    0.5390      1141\n",
      "\n",
      "Aspect Classification Report\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "        FACILITIES#CLEANLINESS     0.0366    0.0600    0.0455        50\n",
      "            FACILITIES#COMFORT     0.0300    0.0600    0.0400        50\n",
      "    FACILITIES#DESIGN&FEATURES     0.0000    0.0000    0.0000        50\n",
      "            FACILITIES#GENERAL     0.0300    0.0600    0.0400        50\n",
      "      FACILITIES#MISCELLANEOUS     0.0385    0.0400    0.0392        50\n",
      "             FACILITIES#PRICES     0.0300    0.0600    0.0400        50\n",
      "            FACILITIES#QUALITY     0.0300    0.0600    0.0400        50\n",
      "     FOOD&DRINKS#MISCELLANEOUS     0.0375    0.0600    0.0462        50\n",
      "            FOOD&DRINKS#PRICES     0.0300    0.0600    0.0400        50\n",
      "           FOOD&DRINKS#QUALITY     0.0000    0.0000    0.0000        50\n",
      "     FOOD&DRINKS#STYLE&OPTIONS     0.0000    0.0000    0.0000        50\n",
      "             HOTEL#CLEANLINESS     0.0300    0.0600    0.0400        50\n",
      "                 HOTEL#COMFORT     0.0300    0.0600    0.0400        50\n",
      "         HOTEL#DESIGN&FEATURES     0.0250    0.0200    0.0222        50\n",
      "                 HOTEL#GENERAL     0.0326    0.0600    0.0423        50\n",
      "           HOTEL#MISCELLANEOUS     0.0000    0.0000    0.0000        50\n",
      "                  HOTEL#PRICES     0.0000    0.0000    0.0000        50\n",
      "                 HOTEL#QUALITY     0.0238    0.0400    0.0299        50\n",
      "              LOCATION#GENERAL     0.0300    0.0600    0.0400        50\n",
      "             ROOMS#CLEANLINESS     0.0000    0.0000    0.0000        50\n",
      "                 ROOMS#COMFORT     0.0000    0.0000    0.0000        50\n",
      "         ROOMS#DESIGN&FEATURES     0.0370    0.0600    0.0458        50\n",
      "                 ROOMS#GENERAL     0.0000    0.0000    0.0000        50\n",
      "           ROOMS#MISCELLANEOUS     0.0000    0.0000    0.0000        50\n",
      "                  ROOMS#PRICES     0.0330    0.0600    0.0426        50\n",
      "                 ROOMS#QUALITY     0.0000    0.0000    0.0000        50\n",
      "    ROOM_AMENITIES#CLEANLINESS     0.0300    0.0600    0.0400        50\n",
      "        ROOM_AMENITIES#COMFORT     0.0000    0.0000    0.0000        50\n",
      "ROOM_AMENITIES#DESIGN&FEATURES     0.0000    0.0000    0.0000        50\n",
      "        ROOM_AMENITIES#GENERAL     0.0303    0.0600    0.0403        50\n",
      "  ROOM_AMENITIES#MISCELLANEOUS     0.0000    0.0000    0.0000        50\n",
      "         ROOM_AMENITIES#PRICES     0.0303    0.0600    0.0403        50\n",
      "        ROOM_AMENITIES#QUALITY     0.0000    0.0000    0.0000        50\n",
      "               SERVICE#GENERAL     0.0000    0.0000    0.0000        50\n",
      "\n",
      "                      accuracy                         0.0312      1700\n",
      "                     macro avg     0.0175    0.0312    0.0222      1700\n",
      "                  weighted avg     0.0175    0.0312    0.0222      1700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh vÃ  tokenizer\n",
    "model_name = \"bert-base-uncased\"  # Sá»­ dá»¥ng mÃ´ hÃ¬nh BERT\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=34*4)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Äá»‹nh nghÄ©a cÃ¡c danh má»¥c tá»« dataframe Ä‘Ã£ unpivot\n",
    "categories = ['FACILITIES#CLEANLINESS', 'FACILITIES#COMFORT', 'FACILITIES#DESIGN&FEATURES', \n",
    "              'FACILITIES#GENERAL', 'FACILITIES#MISCELLANEOUS', 'FACILITIES#PRICES', \n",
    "              'FACILITIES#QUALITY', 'FOOD&DRINKS#MISCELLANEOUS', 'FOOD&DRINKS#PRICES', \n",
    "              'FOOD&DRINKS#QUALITY', 'FOOD&DRINKS#STYLE&OPTIONS', 'HOTEL#CLEANLINESS', \n",
    "              'HOTEL#COMFORT', 'HOTEL#DESIGN&FEATURES', 'HOTEL#GENERAL', 'HOTEL#MISCELLANEOUS', \n",
    "              'HOTEL#PRICES', 'HOTEL#QUALITY', 'LOCATION#GENERAL', 'ROOM_AMENITIES#CLEANLINESS', \n",
    "              'ROOM_AMENITIES#COMFORT', 'ROOM_AMENITIES#DESIGN&FEATURES', 'ROOM_AMENITIES#GENERAL', \n",
    "              'ROOM_AMENITIES#MISCELLANEOUS', 'ROOM_AMENITIES#PRICES', 'ROOM_AMENITIES#QUALITY', \n",
    "              'ROOMS#CLEANLINESS', 'ROOMS#COMFORT', 'ROOMS#DESIGN&FEATURES', 'ROOMS#GENERAL', \n",
    "              'ROOMS#MISCELLANEOUS', 'ROOMS#PRICES', 'ROOMS#QUALITY', 'SERVICE#GENERAL']\n",
    "# HÃ m chia vÄƒn báº£n dÃ i thÃ nh cÃ¡c Ä‘oáº¡n ngáº¯n hÆ¡n\n",
    "# HÃ m chia vÄƒn báº£n dÃ i thÃ nh cÃ¡c Ä‘oáº¡n ngáº¯n hÆ¡n\n",
    "def split_text(text, max_length=128):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+max_length]) for i in range(0, len(words), max_length)]\n",
    "    return chunks\n",
    "\n",
    "# HÃ m dá»± Ä‘oÃ¡n cho tá»«ng Ä‘oáº¡n vÄƒn báº£n\n",
    "def predict(model, tokenizer, sentences, max_length=64):\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        predictions = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# HÃ m decode_predictions Ä‘á»ƒ giáº£i mÃ£ cÃ¡c dá»± Ä‘oÃ¡n vÃ  tráº£ vá» tá»« Ä‘iá»ƒn cÃ¡c aspect vÃ  sentiment tÆ°Æ¡ng á»©ng\n",
    "def decode_predictions(predicted_labels, threshold=0.5):\n",
    "    sentiments = ['positive', 'negative', 'neutral', 'none']\n",
    "    decoded_predictions = {}\n",
    "    for labels in predicted_labels:\n",
    "        for i, pred in enumerate(labels):\n",
    "            if pred > threshold:\n",
    "                sentiment = sentiments[i % 4]\n",
    "                if sentiment in ['neutral', 'none']:\n",
    "                    continue  # Bá» qua náº¿u sentiment lÃ  neutral hoáº·c none\n",
    "                category_index = i // 4\n",
    "                if category_index < len(categories):\n",
    "                    category = categories[category_index]\n",
    "                    if category not in decoded_predictions or pred > decoded_predictions[category][1]:\n",
    "                        decoded_predictions[category] = (sentiment, pred)\n",
    "    return decoded_predictions\n",
    "\n",
    "# Giáº£ sá»­ cá»™t chá»©a cÃ¢u vÄƒn báº£n lÃ  'Review' vÃ  nhÃ£n thá»±c táº¿ lÃ  'Aspect'\n",
    "\n",
    "test_sentences = df_test['Review'].tolist()\n",
    "y_test = df_test['Aspect'].tolist()  # Cá»™t chá»©a nhÃ£n thá»±c táº¿\n",
    "\n",
    "# Xá»­ lÃ½ dá»¯ liá»‡u theo batch nhá»\n",
    "batch_size = 4  # Giáº£m kÃ­ch thÆ°á»›c batch\n",
    "all_predictions = []\n",
    "y_pred = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    example_sentence = text_preprocess(sentence)\n",
    "    chunks = split_text(example_sentence, max_length=32)  # Giáº£m max_length\n",
    "    predictions = []\n",
    "    for chunk in chunks:\n",
    "        predicted_labels = predict(model, tokenizer, [chunk])\n",
    "        predictions.append(predicted_labels[0])  # Láº¥y káº¿t quáº£ Ä‘áº§u tiÃªn cá»§a batch\n",
    "    merged_predictions = np.mean(predictions, axis=0)  # TÃ­nh trung bÃ¬nh cÃ¡c dá»± Ä‘oÃ¡n\n",
    "    decoded_predictions = decode_predictions([merged_predictions])\n",
    "    all_predictions.append(decoded_predictions)\n",
    "    y_pred.append(merged_predictions)  # LÆ°u láº¡i káº¿t quáº£ dá»± Ä‘oÃ¡n thÃ´ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡\n",
    "\n",
    "# Thu tháº­p nhÃ£n thá»±c táº¿ vÃ  nhÃ£n dá»± Ä‘oÃ¡n cho sentiment\n",
    "aspect_test = []\n",
    "aspect_pred = []\n",
    "sentiment_test = []\n",
    "sentiment_pred = []\n",
    "\n",
    "for row_test, row_pred in zip(y_test, y_pred):\n",
    "    row_test_labels = row_test.split(\"; \")\n",
    "    for index, col_pred in enumerate(row_pred):\n",
    "        if index < len(categories):  # Kiá»ƒm tra giá»›i háº¡n chá»‰ sá»‘\n",
    "            if categories[index] in row_test_labels:\n",
    "                aspect_test.append(categories[index])  # NhÃ£n thá»±c táº¿\n",
    "            if col_pred > 0.5:\n",
    "                aspect_pred.append(categories[index])  # NhÃ£n dá»± Ä‘oÃ¡n\n",
    "    decoded_predictions = decode_predictions([row_pred])\n",
    "    for category, (sentiment, _) in decoded_predictions.items():\n",
    "        if category in row_test_labels:\n",
    "            sentiment_test.append(sentiment)  # Sentiment thá»±c táº¿ (náº¿u cÃ³ thÃ´ng tin)\n",
    "        sentiment_pred.append(sentiment)  # Sentiment dá»± Ä‘oÃ¡n\n",
    "\n",
    "# Äáº£m báº£o ráº±ng sá»‘ lÆ°á»£ng pháº§n tá»­ trong sentiment_test vÃ  sentiment_pred lÃ  nháº¥t quÃ¡n\n",
    "min_length = min(len(sentiment_test), len(sentiment_pred))\n",
    "sentiment_test = sentiment_test[:min_length]\n",
    "sentiment_pred = sentiment_pred[:min_length]\n",
    "\n",
    "# Äáº£m báº£o ráº±ng sá»‘ lÆ°á»£ng pháº§n tá»­ trong aspect_test vÃ  aspect_pred lÃ  nháº¥t quÃ¡n\n",
    "min_length_aspect = min(len(aspect_test), len(aspect_pred))\n",
    "aspect_test = aspect_test[:min_length_aspect]\n",
    "aspect_pred = aspect_pred[:min_length_aspect]\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ hiá»‡u quáº£ mÃ´ hÃ¬nh cho sentiment\n",
    "if len(sentiment_test) > 0 and len(sentiment_pred) > 0:  # Kiá»ƒm tra danh sÃ¡ch khÃ´ng rá»—ng\n",
    "    sentiment_report = classification_report(sentiment_test, sentiment_pred, digits=4, output_dict=True)\n",
    "    print(\"Sentiment Classification Report\")\n",
    "    print(classification_report(sentiment_test, sentiment_pred, digits=4))  # Báº£ng Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ mÃ´ hÃ¬nh\n",
    "else:\n",
    "    print(\"No sentiment data to evaluate.\")\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ hiá»‡u quáº£ mÃ´ hÃ¬nh cho aspect\n",
    "if len(aspect_test) > 0 and len(aspect_pred) > 0:  # Kiá»ƒm tra danh sÃ¡ch khÃ´ng rá»—ng\n",
    "    aspect_report = classification_report(aspect_test, aspect_pred, digits=4, zero_division=1, output_dict=True)\n",
    "    print(\"Aspect Classification Report\")\n",
    "    print(classification_report(aspect_test, aspect_pred, digits=4))  # Báº£ng Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ mÃ´ hÃ¬nh\n",
    "else:\n",
    "    print(\"No aspect data to evaluate.\")\n",
    "\n",
    "# Táº¡o DataFrame má»›i chá»©a review vÃ  dá»± Ä‘oÃ¡n\n",
    "df_predictions = pd.DataFrame({\n",
    "    'Review': test_sentences,\n",
    "    'Actual_Aspects': y_test,\n",
    "    'Predicted_Sentiments': [\"; \".join([f\"{cat}: {sent}\" for cat, (sent, _) in decode_predictions([pred]).items()]) for pred in y_pred]\n",
    "})\n",
    "\n",
    "# Äiá»u chá»‰nh tÃ¹y chá»n hiá»ƒn thá»‹ cá»§a pandas Ä‘á»ƒ hiá»ƒn thá»‹ Ä‘áº§y Ä‘á»§ cá»™t 'Review'\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9900c10-a762-490b-b411-ecf6f10181ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
