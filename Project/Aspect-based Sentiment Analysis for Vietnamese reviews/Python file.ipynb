{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07686ef-6db7-4c8d-a051-240809c92cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from nltk import flatten\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from nltk import flatten\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d95d62-a194-4458-9156-e9916425eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e4a3810-ef4d-4829-9b97-7bd875403935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_HTML(text):\n",
    "    return re.sub(r'<[^>]*>', '', text) # Thay thế các ký tự ở param 1 bằng ký tự ở param 2 trong chuỗi ký tự text\n",
    "\n",
    "\n",
    "# Chuẩn hoá unicode, chuyển các ký tự từ mã window 1252 về dạng utf8\n",
    "def convert_unicode(text):\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "    charutf8 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
    "    char1252 = char1252.split('|')\n",
    "    charutf8 = charutf8.split('|')\n",
    "\n",
    "    dic = {}\n",
    "    for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dic[x.group()], text\n",
    "    )\n",
    "\n",
    "\n",
    "# Chuẩn hoá cách typing. Trả ra từ điển vowels_to_ids để tìm ra vị trí của bất kỳ nguyên âm có dấu nào trong bảng vowels_table và thực hiện các thao tác xử lý văn bản tiếp theo dựa trên đó\n",
    "vowels_to_ids = {}\n",
    "vowels_table = [\n",
    "    ['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a' ],\n",
    "    ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "    ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "    ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e' ],\n",
    "    ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "    ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i' ],\n",
    "    ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o' ],\n",
    "    ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "    ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "    ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u' ],\n",
    "    ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "    ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y' ]\n",
    "]\n",
    "\n",
    "for i in range(len(vowels_table)):\n",
    "    for j in range(len(vowels_table[i]) - 1):\n",
    "        vowels_to_ids[vowels_table[i][j]] = (i, j)\n",
    "\n",
    "#Kiểm tra có phải chữ tiếng Việt không dựa trên từ điển vowels_to_ids\n",
    "def is_valid_vietnamese_word(word):\n",
    "    chars = list(word)\n",
    "    vowel_indexes = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if vowel_indexes == -1: vowel_indexes = index\n",
    "            else:\n",
    "                if index - vowel_indexes != 1: return False\n",
    "                vowel_indexes = index\n",
    "    return True\n",
    "\n",
    "# Chuẩn hoá chữ viết\n",
    "def standardize_word_typing(word):\n",
    "    if not is_valid_vietnamese_word(word): return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    vowel_indexes = []\n",
    "    qu_or_gi = False\n",
    "\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        elif x == 9:  # check phụ âm 'qu'\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check phụ âm 'gi'\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = vowels_table[x][0]\n",
    "\n",
    "        if not qu_or_gi or index != 1:\n",
    "            vowel_indexes.append(index)\n",
    "\n",
    "    if len(vowel_indexes) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = vowels_to_ids.get(chars[1])\n",
    "                chars[1] = vowels_table[x][dau_cau]\n",
    "            else:\n",
    "                x, y = vowels_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1: chars[2] = vowels_table[x][dau_cau]\n",
    "                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in vowel_indexes:\n",
    "        x, y = vowels_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = vowels_table[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(vowel_indexes) == 2:\n",
    "        if vowel_indexes[-1] == len(chars) - 1:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[0]]]\n",
    "            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]\n",
    "        else:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    else:\n",
    "        x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "# Chuẩn hoá câu viết\n",
    "def standardize_sentence_typing(text):\n",
    "    words = text.lower().split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "replace_list = {\n",
    "    'ô kêi': 'ok', 'okie': 'ok', 'o kê': 'ok', 'okey': 'ok', 'ôkê': 'ok', 'oki': 'ok', 'oke': 'ok', 'okay': 'ok', 'okê': 'ok',\n",
    "    'tks': 'cảm ơn', 'thks': 'cảm ơn', 'thanks': 'cảm ơn', 'ths': 'cảm ơn', 'thank': 'cảm ơn',\n",
    "    'kg': 'không', 'not': 'không', 'k': 'không', 'kh': 'không', 'kô': 'không', 'hok': 'không', 'ko': 'không', 'khong': 'không', 'kp': 'không phải',\n",
    "    'he he': 'tích cực', 'hehe': 'tích cực', 'hihi': 'tích cực', 'haha': 'tích cực', 'hjhj': 'tích cực', 'thick': 'tích cực',\n",
    "    'lol': 'tiêu cực', 'cc': 'tiêu cực', 'huhu': 'tiêu cực', 'cute': 'dễ thương',\n",
    "\n",
    "    'sz': 'cỡ', 'size': 'cỡ',\n",
    "    'wa': 'quá', 'wá': 'quá', 'qá': 'quá',\n",
    "    'đx': 'được', 'dk': 'được', 'dc': 'được', 'đk': 'được', 'đc': 'được',\n",
    "    'vs': 'với', 'j': 'gì', '“': ' ', 'time': 'thời gian', 'm': 'mình', 'mik': 'mình', 'r': 'rồi', 'bjo': 'bao giờ', 'very': 'rất',\n",
    "\n",
    "    'authentic': 'chuẩn chính hãng', 'aut': 'chuẩn chính hãng', 'auth': 'chuẩn chính hãng', 'date': 'hạn sử dụng', 'hsd': 'hạn sử dụng',\n",
    "    'store': 'cửa hàng', 'sop': 'cửa hàng', 'shopE': 'cửa hàng', 'shop': 'cửa hàng',\n",
    "    'sp': 'sản phẩm', 'product': 'sản phẩm', 'hàg': 'hàng',\n",
    "    'ship': 'giao hàng', 'delivery': 'giao hàng', 'síp': 'giao hàng', 'order': 'đặt hàng',\n",
    "\n",
    "    'gud': 'tốt', 'wel done': 'tốt', 'good': 'tốt', 'gút': 'tốt', 'tot': 'tốt', 'nice': 'tốt', 'perfect': 'rất tốt',\n",
    "    'quality': 'chất lượng', 'chất lg': 'chất lượng', 'chat': 'chất', 'excelent': 'hoàn hảo', 'bt': 'bình thường',\n",
    "    'sad': 'tệ', 'por': 'tệ', 'poor': 'tệ', 'bad': 'tệ',\n",
    "    'beautiful': 'đẹp tuyệt vời', 'dep': 'đẹp',\n",
    "    'xau': 'xấu', 'sấu': 'xấu',\n",
    "\n",
    "    'thik': 'thích', 'iu': 'yêu', 'fake': 'giả mạo',\n",
    "    'quickly': 'nhanh', 'quick': 'nhanh', 'fast': 'nhanh',\n",
    "    'fresh': 'tươi', 'delicious': 'ngon',\n",
    "\n",
    "    'dt': 'điện thoại', 'fb': 'facebook', 'face': 'facebook', 'ks': 'khách sạn', 'nv': 'nhân viên',\n",
    "    'nt': 'nhắn tin', 'ib': 'nhắn tin', 'tl': 'trả lời', 'trl': 'trả lời', 'rep': 'trả lời',\n",
    "    'fback': 'feedback', 'fedback': 'feedback',\n",
    "    'sd': 'sử dụng', 'sài': 'xài',\n",
    "\n",
    "    '^_^': 'tích cực', ':)': 'tích cực', ':(': 'tiêu cực',\n",
    "    '❤️': 'tích cực', '👍': 'tích cực', '🎉': 'tích cực', '😀': 'tích cực', '😍': 'tích cực', '😂': 'tích cực', '🤗': 'tích cực', '😙': 'tích cực', '🙂': 'tích cực',\n",
    "    '😔': 'tiêu cực', '😓': 'tiêu cực',\n",
    "    '⭐': 'star', '*': 'star', '🌟': 'star',\n",
    "}\n",
    "\n",
    "# Chuẩn hoá các từ viết tắt, ký tự thành tiếng Việt dựa trên danh sách replace_list\n",
    "def normalize_acronyms(text):\n",
    "    words = []\n",
    "    for word in text.strip().split():\n",
    "        # word = word.strip(string.punctuation)\n",
    "        if word.lower() not in replace_list.keys(): words.append(word)\n",
    "        else: words.append(replace_list[word.lower()])\n",
    "    return emoji.demojize(' '.join(words)) # Remove Emojis\n",
    "\n",
    "\n",
    "# Word segmentation: Trả ra một chuỗi văn bản với các từ đã được phân tách và nối lại với nhau bằng khoảng trắng, sử dụng nguồn file chuẩn hoá VnCoreNLP-1.1.1.jar\n",
    "#annotator = VnCoreNLP('VnCoreNLP/VnCoreNLP-1.1.1.jar')\n",
    "#def word_segmentation(text):\n",
    "#    words = annotator.tokenize(text)\n",
    "#    return ' '.join(word for word in flatten(words))\n",
    "\n",
    "\n",
    "# Loại bỏ các ký tự không cần thiết trong văn bản\n",
    "def remove_unnecessary_characters(text):\n",
    "    text = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬÉÈẺẼẸÊẾỀỂỄỆÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÍÌỈĨỊÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐ_]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text\n",
    "# Tạo function tổng hợp xử lý 1 văn bản\n",
    "def text_preprocess(text):\n",
    "    text = remove_HTML(text)\n",
    "    text = convert_unicode(text)\n",
    "    # text = standardize_sentence_typing(text)\n",
    "    text = normalize_acronyms(text)\n",
    "    #text = word_segmentation(text) # When use PhoBERT\n",
    "    text = remove_unnecessary_characters(text)\n",
    "    # return text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "941496fe-057c-4fa9-aff5-5559c3c1184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Admin\\Documents\\Zalo Received Files\\VLSP_Hotel'\n",
    "os.chdir(path)\n",
    "TRAIN_PATH = 'hotel_train.csv'\n",
    "VAL_PATH = 'hotel_val.csv'\n",
    "TEST_PATH = 'hotel_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d6096b-234f-4a49-aabd-32703715bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_PATH).head(200)\n",
    "df_val = pd.read_csv(VAL_PATH).head(50)\n",
    "df_test = pd.read_csv(TEST_PATH).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68341003-c111-4806-b408-b39d3a155e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['Review'] = df_val['Review'].apply(lambda x: text_preprocess(str(x)))\n",
    "df_test['Review'] = df_test['Review'].apply(lambda x: text_preprocess(str(x)))\n",
    "df_train['Review'] = df_train['Review'].apply(lambda x: text_preprocess(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0699e9b-7641-4bec-8f6f-f95361529ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot_and_filter(df):\n",
    "    # Unpivot DataFrame\n",
    "    df_unpivoted = pd.melt(df, id_vars=['Review'], var_name='Aspect', value_name='Sentiment')\n",
    "    \n",
    "    # Loại bỏ các dòng có sentiment \"none\"\n",
    "    df_filtered = df_unpivoted #[df_unpivoted['Sentiment'] != 0]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Áp dụng function cho các DataFrame\n",
    "df_train = unpivot_and_filter(df_train).reset_index()\n",
    "df_val = unpivot_and_filter(df_val).reset_index()\n",
    "df_test = unpivot_and_filter(df_test).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "400e0133-72ba-4465-bbba-2968dbdaf9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = str(self.data.loc[index, 'Review'])\n",
    "        aspect = str(self.data.loc[index, 'Aspect'])\n",
    "        sentiment = self.data.loc[index, 'Sentiment']\n",
    "        \n",
    "        # Encode sentiment\n",
    "        if sentiment == 1: one_hot = [0, 1, 0, 0] # Positive\n",
    "        elif sentiment == 2: one_hot = [0, 0, 1, 0] # Negatvie\n",
    "        elif sentiment == 3: one_hot = [0, 0, 0, 1] # Neutral\n",
    "        else: one_hot = [1, 0, 0, 0]  # Default to none if sentiment is missing or incorrect\n",
    "            \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review + \" \" + aspect,  # Concatenate review and aspect\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(one_hot, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Tạo DataLoader\n",
    "batch_size = 4\n",
    "max_len = 256\n",
    "train_dataset = CustomDataset(df_train, tokenizer, max_len=max_len)\n",
    "val_dataset = CustomDataset(df_val, tokenizer, max_len=max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dedd070e-cc26-40dc-93b2-2f913c19609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, pooled_output = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.classifier(output)\n",
    "\n",
    "model = CustomModel(num_labels=4)  # Adjust num_labels to 4 for Positive, Negative, Neutral, None\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Move model to the device\n",
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87497a7e-428a-4128-ae79-9f69f03f3435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   565, 10456,  ...,     1,     1,     1],\n",
      "        [    0,   448,  3849,  ...,  3070,  6248,     2],\n",
      "        [    0,   487,   298,  ...,     1,     1,     1],\n",
      "        [    0, 17297,  3849,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "# In ra một batch để kiểm tra\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87776c3d-b7b6-4162-926e-4a2fe0a4ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    train_loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 10 == 0:  # Print every 10 batches\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] finished with average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return train_loss_history\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss_history = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    val_loss_history.append(epoch_val_loss)\n",
    "    print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    return val_loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4d8e2-2c32-45b8-bffb-59761a15524d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577ac8d2-63a5-4aa9-bb7f-0bda4aa4262d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_loss_history \u001b[38;5;241m=\u001b[39m train(model, train_loader, criterion, optimizer, epochs)\n\u001b[0;32m      3\u001b[0m val_loss_history \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask, token_type_ids)\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, token_type_ids):\n\u001b[1;32m---> 12\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[0;32m     13\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m     14\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m     15\u001b[0m         token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m     16\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(pooled_output)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(output)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:828\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    819\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    821\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    822\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    823\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    826\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    827\u001b[0m )\n\u001b[1;32m--> 828\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    829\u001b[0m     embedding_output,\n\u001b[0;32m    830\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    831\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    832\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    833\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m    834\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    835\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    836\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    837\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    838\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    839\u001b[0m )\n\u001b[0;32m    840\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    841\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:517\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    506\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    507\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    508\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m         output_attentions,\n\u001b[0;32m    515\u001b[0m     )\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 517\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    518\u001b[0m         hidden_states,\n\u001b[0;32m    519\u001b[0m         attention_mask,\n\u001b[0;32m    520\u001b[0m         layer_head_mask,\n\u001b[0;32m    521\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    522\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    523\u001b[0m         past_key_value,\n\u001b[0;32m    524\u001b[0m         output_attentions,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    527\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:406\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    396\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    407\u001b[0m         hidden_states,\n\u001b[0;32m    408\u001b[0m         attention_mask,\n\u001b[0;32m    409\u001b[0m         head_mask,\n\u001b[0;32m    410\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    411\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    412\u001b[0m     )\n\u001b[0;32m    413\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:333\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    325\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    332\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 333\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    334\u001b[0m         hidden_states,\n\u001b[0;32m    335\u001b[0m         attention_mask,\n\u001b[0;32m    336\u001b[0m         head_mask,\n\u001b[0;32m    337\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    338\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    339\u001b[0m         past_key_value,\n\u001b[0;32m    340\u001b[0m         output_attentions,\n\u001b[0;32m    341\u001b[0m     )\n\u001b[0;32m    342\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    343\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:259\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    256\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1813\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1809\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m-> 1813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   1814\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies a softmax function.\u001b[39;00m\n\u001b[0;32m   1815\u001b[0m \n\u001b[0;32m   1816\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1836\u001b[0m \n\u001b[0;32m   1837\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "train_loss_history = train(model, train_loader, criterion, optimizer, epochs)\n",
    "val_loss_history = evaluate(model, val_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "914208cf-0503-4b73-be86-d33ab90487db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = 'model_weights.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70b245-64a6-4583-ba0e-40c4597806c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import re\n",
    "model_name = \"bert-base-uncased\"  # Sử dụng mô hình BERT\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=34*4)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Định nghĩa các danh mục từ dataframe đã unpivot\n",
    "categories = ['FACILITIES#CLEANLINESS', 'FACILITIES#COMFORT', 'FACILITIES#DESIGN&FEATURES', \n",
    "              'FACILITIES#GENERAL', 'FACILITIES#MISCELLANEOUS', 'FACILITIES#PRICES', \n",
    "              'FACILITIES#QUALITY', 'FOOD&DRINKS#MISCELLANEOUS', 'FOOD&DRINKS#PRICES', \n",
    "              'FOOD&DRINKS#QUALITY', 'FOOD&DRINKS#STYLE&OPTIONS', 'HOTEL#CLEANLINESS', \n",
    "              'HOTEL#COMFORT', 'HOTEL#DESIGN&FEATURES', 'HOTEL#GENERAL', 'HOTEL#MISCELLANEOUS', \n",
    "              'HOTEL#PRICES', 'HOTEL#QUALITY', 'LOCATION#GENERAL', 'ROOM_AMENITIES#CLEANLINESS', \n",
    "              'ROOM_AMENITIES#COMFORT', 'ROOM_AMENITIES#DESIGN&FEATURES', 'ROOM_AMENITIES#GENERAL', \n",
    "              'ROOM_AMENITIES#MISCELLANEOUS', 'ROOM_AMENITIES#PRICES', 'ROOM_AMENITIES#QUALITY', \n",
    "              'ROOMS#CLEANLINESS', 'ROOMS#COMFORT', 'ROOMS#DESIGN&FEATURES', 'ROOMS#GENERAL', \n",
    "              'ROOMS#MISCELLANEOUS', 'ROOMS#PRICES', 'ROOMS#QUALITY', 'SERVICE#GENERAL']\n",
    "# Hàm dự đoán\n",
    "def predict(model, tokenizer, sentence):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        max_length=128,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        predictions = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Hàm decode_predictions để giải mã các dự đoán và trả về danh sách các aspect và sentiment tương ứng\n",
    "def decode_predictions(predicted_labels, threshold=0.5):\n",
    "    sentiments = ['positive', 'negative', 'neutral', 'none']\n",
    "    decoded_predictions = {}\n",
    "    for i, pred in enumerate(predicted_labels[0]):\n",
    "        if pred > threshold:\n",
    "            sentiment = sentiments[i % 4]\n",
    "            if sentiment in ['neutral', 'none']:\n",
    "                continue  # Bỏ qua nếu sentiment là neutral hoặc none\n",
    "            category_index = i // 4\n",
    "            category = categories[category_index]\n",
    "            if category not in decoded_predictions or pred > decoded_predictions[category][1]:\n",
    "                decoded_predictions[category] = (sentiment, pred)\n",
    "    \n",
    "    return [f\"{category}: {sentiment}\" for category, (sentiment, _) in decoded_predictions.items()]\n",
    "\n",
    "# Nhận văn bản đầu vào từ người dùng\n",
    "input_text = input(\"Nhập văn bản: \")\n",
    "example_sentence = text_preprocess(input_text)\n",
    "\n",
    "# Dự đoán nhãn\n",
    "predicted_labels = predict(model, tokenizer, example_sentence)\n",
    "\n",
    "# Giải mã dự đoán\n",
    "decoded_predictions = decode_predictions(predicted_labels)\n",
    "\n",
    "# Hiển thị kết quả dự đoán\n",
    "print(f\"Example: {input_text}\")\n",
    "for prediction in decoded_predictions:\n",
    "    print(f\"=> {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3da8dfe-d205-4da7-8f86-c17a956803ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6431    0.6431    0.6431       737\n",
      "    positive     0.3490    0.3490    0.3490       404\n",
      "\n",
      "    accuracy                         0.5390      1141\n",
      "   macro avg     0.4961    0.4961    0.4961      1141\n",
      "weighted avg     0.5390    0.5390    0.5390      1141\n",
      "\n",
      "Aspect Classification Report\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "        FACILITIES#CLEANLINESS     0.0366    0.0600    0.0455        50\n",
      "            FACILITIES#COMFORT     0.0300    0.0600    0.0400        50\n",
      "    FACILITIES#DESIGN&FEATURES     0.0000    0.0000    0.0000        50\n",
      "            FACILITIES#GENERAL     0.0300    0.0600    0.0400        50\n",
      "      FACILITIES#MISCELLANEOUS     0.0385    0.0400    0.0392        50\n",
      "             FACILITIES#PRICES     0.0300    0.0600    0.0400        50\n",
      "            FACILITIES#QUALITY     0.0300    0.0600    0.0400        50\n",
      "     FOOD&DRINKS#MISCELLANEOUS     0.0375    0.0600    0.0462        50\n",
      "            FOOD&DRINKS#PRICES     0.0300    0.0600    0.0400        50\n",
      "           FOOD&DRINKS#QUALITY     0.0000    0.0000    0.0000        50\n",
      "     FOOD&DRINKS#STYLE&OPTIONS     0.0000    0.0000    0.0000        50\n",
      "             HOTEL#CLEANLINESS     0.0300    0.0600    0.0400        50\n",
      "                 HOTEL#COMFORT     0.0300    0.0600    0.0400        50\n",
      "         HOTEL#DESIGN&FEATURES     0.0250    0.0200    0.0222        50\n",
      "                 HOTEL#GENERAL     0.0326    0.0600    0.0423        50\n",
      "           HOTEL#MISCELLANEOUS     0.0000    0.0000    0.0000        50\n",
      "                  HOTEL#PRICES     0.0000    0.0000    0.0000        50\n",
      "                 HOTEL#QUALITY     0.0238    0.0400    0.0299        50\n",
      "              LOCATION#GENERAL     0.0300    0.0600    0.0400        50\n",
      "             ROOMS#CLEANLINESS     0.0000    0.0000    0.0000        50\n",
      "                 ROOMS#COMFORT     0.0000    0.0000    0.0000        50\n",
      "         ROOMS#DESIGN&FEATURES     0.0370    0.0600    0.0458        50\n",
      "                 ROOMS#GENERAL     0.0000    0.0000    0.0000        50\n",
      "           ROOMS#MISCELLANEOUS     0.0000    0.0000    0.0000        50\n",
      "                  ROOMS#PRICES     0.0330    0.0600    0.0426        50\n",
      "                 ROOMS#QUALITY     0.0000    0.0000    0.0000        50\n",
      "    ROOM_AMENITIES#CLEANLINESS     0.0300    0.0600    0.0400        50\n",
      "        ROOM_AMENITIES#COMFORT     0.0000    0.0000    0.0000        50\n",
      "ROOM_AMENITIES#DESIGN&FEATURES     0.0000    0.0000    0.0000        50\n",
      "        ROOM_AMENITIES#GENERAL     0.0303    0.0600    0.0403        50\n",
      "  ROOM_AMENITIES#MISCELLANEOUS     0.0000    0.0000    0.0000        50\n",
      "         ROOM_AMENITIES#PRICES     0.0303    0.0600    0.0403        50\n",
      "        ROOM_AMENITIES#QUALITY     0.0000    0.0000    0.0000        50\n",
      "               SERVICE#GENERAL     0.0000    0.0000    0.0000        50\n",
      "\n",
      "                      accuracy                         0.0312      1700\n",
      "                     macro avg     0.0175    0.0312    0.0222      1700\n",
      "                  weighted avg     0.0175    0.0312    0.0222      1700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "# Định nghĩa mô hình và tokenizer\n",
    "model_name = \"bert-base-uncased\"  # Sử dụng mô hình BERT\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=34*4)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Định nghĩa các danh mục từ dataframe đã unpivot\n",
    "categories = ['FACILITIES#CLEANLINESS', 'FACILITIES#COMFORT', 'FACILITIES#DESIGN&FEATURES', \n",
    "              'FACILITIES#GENERAL', 'FACILITIES#MISCELLANEOUS', 'FACILITIES#PRICES', \n",
    "              'FACILITIES#QUALITY', 'FOOD&DRINKS#MISCELLANEOUS', 'FOOD&DRINKS#PRICES', \n",
    "              'FOOD&DRINKS#QUALITY', 'FOOD&DRINKS#STYLE&OPTIONS', 'HOTEL#CLEANLINESS', \n",
    "              'HOTEL#COMFORT', 'HOTEL#DESIGN&FEATURES', 'HOTEL#GENERAL', 'HOTEL#MISCELLANEOUS', \n",
    "              'HOTEL#PRICES', 'HOTEL#QUALITY', 'LOCATION#GENERAL', 'ROOM_AMENITIES#CLEANLINESS', \n",
    "              'ROOM_AMENITIES#COMFORT', 'ROOM_AMENITIES#DESIGN&FEATURES', 'ROOM_AMENITIES#GENERAL', \n",
    "              'ROOM_AMENITIES#MISCELLANEOUS', 'ROOM_AMENITIES#PRICES', 'ROOM_AMENITIES#QUALITY', \n",
    "              'ROOMS#CLEANLINESS', 'ROOMS#COMFORT', 'ROOMS#DESIGN&FEATURES', 'ROOMS#GENERAL', \n",
    "              'ROOMS#MISCELLANEOUS', 'ROOMS#PRICES', 'ROOMS#QUALITY', 'SERVICE#GENERAL']\n",
    "# Hàm chia văn bản dài thành các đoạn ngắn hơn\n",
    "# Hàm chia văn bản dài thành các đoạn ngắn hơn\n",
    "def split_text(text, max_length=128):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+max_length]) for i in range(0, len(words), max_length)]\n",
    "    return chunks\n",
    "\n",
    "# Hàm dự đoán cho từng đoạn văn bản\n",
    "def predict(model, tokenizer, sentences, max_length=64):\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        sentences,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        predictions = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Hàm decode_predictions để giải mã các dự đoán và trả về từ điển các aspect và sentiment tương ứng\n",
    "def decode_predictions(predicted_labels, threshold=0.5):\n",
    "    sentiments = ['positive', 'negative', 'neutral', 'none']\n",
    "    decoded_predictions = {}\n",
    "    for labels in predicted_labels:\n",
    "        for i, pred in enumerate(labels):\n",
    "            if pred > threshold:\n",
    "                sentiment = sentiments[i % 4]\n",
    "                if sentiment in ['neutral', 'none']:\n",
    "                    continue  # Bỏ qua nếu sentiment là neutral hoặc none\n",
    "                category_index = i // 4\n",
    "                if category_index < len(categories):\n",
    "                    category = categories[category_index]\n",
    "                    if category not in decoded_predictions or pred > decoded_predictions[category][1]:\n",
    "                        decoded_predictions[category] = (sentiment, pred)\n",
    "    return decoded_predictions\n",
    "\n",
    "# Giả sử cột chứa câu văn bản là 'Review' và nhãn thực tế là 'Aspect'\n",
    "\n",
    "test_sentences = df_test['Review'].tolist()\n",
    "y_test = df_test['Aspect'].tolist()  # Cột chứa nhãn thực tế\n",
    "\n",
    "# Xử lý dữ liệu theo batch nhỏ\n",
    "batch_size = 4  # Giảm kích thước batch\n",
    "all_predictions = []\n",
    "y_pred = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    example_sentence = text_preprocess(sentence)\n",
    "    chunks = split_text(example_sentence, max_length=32)  # Giảm max_length\n",
    "    predictions = []\n",
    "    for chunk in chunks:\n",
    "        predicted_labels = predict(model, tokenizer, [chunk])\n",
    "        predictions.append(predicted_labels[0])  # Lấy kết quả đầu tiên của batch\n",
    "    merged_predictions = np.mean(predictions, axis=0)  # Tính trung bình các dự đoán\n",
    "    decoded_predictions = decode_predictions([merged_predictions])\n",
    "    all_predictions.append(decoded_predictions)\n",
    "    y_pred.append(merged_predictions)  # Lưu lại kết quả dự đoán thô để đánh giá\n",
    "\n",
    "# Thu thập nhãn thực tế và nhãn dự đoán cho sentiment\n",
    "aspect_test = []\n",
    "aspect_pred = []\n",
    "sentiment_test = []\n",
    "sentiment_pred = []\n",
    "\n",
    "for row_test, row_pred in zip(y_test, y_pred):\n",
    "    row_test_labels = row_test.split(\"; \")\n",
    "    for index, col_pred in enumerate(row_pred):\n",
    "        if index < len(categories):  # Kiểm tra giới hạn chỉ số\n",
    "            if categories[index] in row_test_labels:\n",
    "                aspect_test.append(categories[index])  # Nhãn thực tế\n",
    "            if col_pred > 0.5:\n",
    "                aspect_pred.append(categories[index])  # Nhãn dự đoán\n",
    "    decoded_predictions = decode_predictions([row_pred])\n",
    "    for category, (sentiment, _) in decoded_predictions.items():\n",
    "        if category in row_test_labels:\n",
    "            sentiment_test.append(sentiment)  # Sentiment thực tế (nếu có thông tin)\n",
    "        sentiment_pred.append(sentiment)  # Sentiment dự đoán\n",
    "\n",
    "# Đảm bảo rằng số lượng phần tử trong sentiment_test và sentiment_pred là nhất quán\n",
    "min_length = min(len(sentiment_test), len(sentiment_pred))\n",
    "sentiment_test = sentiment_test[:min_length]\n",
    "sentiment_pred = sentiment_pred[:min_length]\n",
    "\n",
    "# Đảm bảo rằng số lượng phần tử trong aspect_test và aspect_pred là nhất quán\n",
    "min_length_aspect = min(len(aspect_test), len(aspect_pred))\n",
    "aspect_test = aspect_test[:min_length_aspect]\n",
    "aspect_pred = aspect_pred[:min_length_aspect]\n",
    "\n",
    "# Đánh giá hiệu quả mô hình cho sentiment\n",
    "if len(sentiment_test) > 0 and len(sentiment_pred) > 0:  # Kiểm tra danh sách không rỗng\n",
    "    sentiment_report = classification_report(sentiment_test, sentiment_pred, digits=4, output_dict=True)\n",
    "    print(\"Sentiment Classification Report\")\n",
    "    print(classification_report(sentiment_test, sentiment_pred, digits=4))  # Bảng đánh giá hiệu quả mô hình\n",
    "else:\n",
    "    print(\"No sentiment data to evaluate.\")\n",
    "\n",
    "# Đánh giá hiệu quả mô hình cho aspect\n",
    "if len(aspect_test) > 0 and len(aspect_pred) > 0:  # Kiểm tra danh sách không rỗng\n",
    "    aspect_report = classification_report(aspect_test, aspect_pred, digits=4, zero_division=1, output_dict=True)\n",
    "    print(\"Aspect Classification Report\")\n",
    "    print(classification_report(aspect_test, aspect_pred, digits=4))  # Bảng đánh giá hiệu quả mô hình\n",
    "else:\n",
    "    print(\"No aspect data to evaluate.\")\n",
    "\n",
    "# Tạo DataFrame mới chứa review và dự đoán\n",
    "df_predictions = pd.DataFrame({\n",
    "    'Review': test_sentences,\n",
    "    'Actual_Aspects': y_test,\n",
    "    'Predicted_Sentiments': [\"; \".join([f\"{cat}: {sent}\" for cat, (sent, _) in decode_predictions([pred]).items()]) for pred in y_pred]\n",
    "})\n",
    "\n",
    "# Điều chỉnh tùy chọn hiển thị của pandas để hiển thị đầy đủ cột 'Review'\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9900c10-a762-490b-b411-ecf6f10181ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
